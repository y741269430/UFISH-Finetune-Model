# UFISH-test

## ç›®å½• ####
- 0.åŠ è½½ç¯å¢ƒ
- 1.æ•°æ®é›†åŠæ¨¡å‹æ¦‚å†µ
- 2.æ•°æ®é›†å¤„ç†æµç¨‹
- 3.UFISH Finetune
- 4.Spotiflow Finetune
- 5.deepBlink Finetune


---
## 0.åŠ è½½ç¯å¢ƒ ####
```
# è¾“å‡ºç¯å¢ƒ
# conda env export > ufish.yml 
# å®‰è£…ç¯å¢ƒ
conda env create -f ufish.yml

conda activate ufish
```
## 1.æ•°æ®é›†åŠæ¨¡å‹æ¦‚å†µ ####

## 2.æ•°æ®é›†å¤„ç†æµç¨‹ ####
### 2.1 ä¾æ®å›¾ç‰‡ä¸Šçš„å‘½åï¼Œæ‹†åˆ†é€šé“    
ä¸¾ä¾‹è¯´æ˜æˆ‘æ˜¯å¦‚ä½•æŠŠä¸€ä¸ªTIFï¼Œæ ¹æ®æ–‡ä»¶åï¼Œæ‹†åˆ†é€šé“çš„     
```python
#### ç¥ç»èŠ‚ ####

from PIL import Image
import os

# å®šä¹‰è·¯å¾„
input_dir = '/home/jjyang/jupyter_file/my_finetune/callç‚¹å›¾åƒæ•´ç†/ç¥ç»èŠ‚/2/bigwarp'  # è¾“å…¥æ–‡ä»¶å¤¹ï¼ˆåŒ…å«å¤šé€šé“ TIF æ–‡ä»¶ï¼‰
output_dir = '/home/jjyang/jupyter_file/my_finetune/callç‚¹å›¾åƒæ•´ç†/ç¥ç»èŠ‚/2/RS-FISH'  # è¾“å‡ºæ–‡ä»¶å¤¹

# åˆ›å»ºè¾“å‡ºæ–‡ä»¶å¤¹ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
os.makedirs(output_dir, exist_ok=True)

# éå†è¾“å…¥æ–‡ä»¶å¤¹ä¸­çš„æ‰€æœ‰ TIF æ–‡ä»¶
for file_name in os.listdir(input_dir):
    if not file_name.endswith('.tif'):
        continue  # è·³è¿‡é TIF æ–‡ä»¶

    tif_path = os.path.join(input_dir, file_name)  # å®Œæ•´è·¯å¾„
    print(f"æ­£åœ¨å¤„ç†æ–‡ä»¶ï¼š{tif_path}")

    try:
        # æ‰“å¼€å¤šé€šé“ TIF æ–‡ä»¶
        img = Image.open(tif_path)

        # è§£æ TIF æ–‡ä»¶åï¼Œæå–åŸºå› åç§°
        base_name, _ = os.path.splitext(file_name)  # å»æ‰æ‰©å±•å
        parts = base_name.split('-')[-1].split(' ')  # æå–æœ€åä¸€ä¸ªéƒ¨åˆ†å¹¶åˆ†å‰²åŸºå› åç§°
        gene_names = [part for part in parts if part]  # è¿‡æ»¤ç©ºå­—ç¬¦ä¸²

        # ç¡®ä¿åŸºå› åç§°æ•°é‡ä¸è¶…è¿‡é€šé“æ•° - 1ï¼ˆå› ä¸ºç¬¬ä¸€ä¸ªé€šé“æ˜¯ DAPIï¼‰
        if len(gene_names) > img.n_frames - 1:
            raise ValueError(f"åŸºå› åç§°æ•°é‡è¶…è¿‡é€šé“æ•°ï¼TIF æ–‡ä»¶ {file_name} åŒ…å« {img.n_frames} ä¸ªé€šé“ï¼Œä½†æ‰¾åˆ° {len(gene_names)} ä¸ªåŸºå› åç§°ã€‚")

        # å¾ªç¯éå†æ‰€æœ‰å¸§/é€šé“
        for i in range(img.n_frames):
            img.seek(i)  # ç§»åŠ¨åˆ°æŒ‡å®šå¸§
            frame = img.copy()

            # ç¡®å®šæ–°æ–‡ä»¶å
            if i == 0:
                new_file_name = "DAPI.tif"  # ç¬¬ä¸€ä¸ªé€šé“æ˜¯ DAPI
            elif i <= len(gene_names):
                gene_name = gene_names[i - 1]  # å¯¹åº”çš„åŸºå› åç§°
                new_file_name = f"{gene_name}.tif"
            else:
                new_file_name = f"Unknown_Ch{i}.tif"  # å¦‚æœåŸºå› åç§°ä¸è¶³ï¼Œåˆ™ä½¿ç”¨é»˜è®¤åç§°

            # ä¿å­˜å›¾ç‰‡
            output_path = os.path.join(output_dir, new_file_name)
            frame.save(output_path)
            print(f"å·²ä¿å­˜ï¼š{output_path}")

    except Exception as e:
        print(f"å¤„ç†æ–‡ä»¶ {tif_path} æ—¶å‡ºé”™ï¼š{e}")
```
ç»“æœå¦‚ä¸‹ï¼š    
<img src="https://github.com/y741269430/UFISH-test/blob/main/Imgs/p1.jpg" width="800" />      

### 2.2 æŠŠTIFä»¥åŠCSVå¤åˆ¶åˆ°æ–°ç›®å½•ä¸‹    
ä¸¾ä¾‹è¯´æ˜æˆ‘æ˜¯å¦‚ä½•æŠŠä¸Šé¢2.1æ‰€æ‹†åˆ†çš„TIFæ–‡ä»¶ï¼Œå¤åˆ¶åˆ°æ–°çš„ç›®å½•ä¸‹ï¼ˆè§„åˆ™æ˜¯ï¼Œæ ¹æ®åŸTIFçš„è·¯å¾„ï¼Œç»„åˆæˆä¸€ä¸ªæ–°æ–‡ä»¶åï¼‰     
```python
import os
import shutil

# å®šä¹‰æºç›®å½•å’Œç›®æ ‡ç›®å½•è·¯å¾„
source_root_dir = '/home/jjyang/jupyter_file/my_finetune/callç‚¹å›¾åƒæ•´ç†/ç¥ç»èŠ‚'
target_dir = '/home/jjyang/jupyter_file/my_finetune/callç‚¹å›¾ç‰‡åŠcsv/'

# åˆ›å»ºç›®æ ‡ç›®å½•ï¼ˆå¦‚æœä¸å­˜åœ¨ï¼‰
os.makedirs(target_dir, exist_ok=True)

def copy_and_rename_files(source_path, target_path):
    try:
        print(f"Copying and renaming files from {source_path} to {target_path}:\n")
        
        # æå–æºç›®å½•çš„å…³é”®ä¿¡æ¯ï¼ˆä¾‹å¦‚çˆ¶æ–‡ä»¶å¤¹åç§°ï¼‰
        parent_folder_name = os.path.basename(os.path.dirname(source_path))  # è·å–ä¸Šä¸€çº§æ–‡ä»¶å¤¹åç§°
        grandparent_folder_name = os.path.basename(os.path.dirname(os.path.dirname(source_path)))  # è·å–ä¸Šä¸¤çº§æ–‡ä»¶å¤¹åç§°
        
        # éå†æºç›®å½•ä¸­çš„æ‰€æœ‰æ–‡ä»¶
        for file_name in os.listdir(source_path):
            source_file_path = os.path.join(source_path, file_name)
            
            # ç­›é€‰ .csv å’Œ .tif æ–‡ä»¶
            if file_name.lower().endswith(('.csv', '.tif')):
                # æå–æ–‡ä»¶åå’Œæ‰©å±•å
                base_name, ext = os.path.splitext(file_name)
                
                # æ„é€ æ–°æ–‡ä»¶å
                new_file_name = f"{grandparent_folder_name}-{parent_folder_name}-{base_name}{ext}"
                target_file_path = os.path.join(target_path, new_file_name)
                
                # å¤åˆ¶å¹¶é‡å‘½åæ–‡ä»¶
                shutil.copy(source_file_path, target_file_path)
                print(f"Copied and renamed: {file_name} -> {new_file_name}")
    
    except FileNotFoundError:
        print(f"The directory {source_path} does not exist.")
    except PermissionError:
        print(f"Permission denied accessing {source_path}.")
    except Exception as e:
        print(f"An error occurred: {e}")

def process_all_subfolders(root_dir, target_dir):
    """
    éå† root_dir ä¸‹çš„æ‰€æœ‰å­æ–‡ä»¶å¤¹ï¼Œå¹¶å¯¹æ¯ä¸ªç¬¦åˆæ¡ä»¶çš„å­æ–‡ä»¶å¤¹è°ƒç”¨ copy_and_rename_filesã€‚
    """
    for subfolder in os.listdir(root_dir):
        subfolder_path = os.path.join(root_dir, subfolder)
        
        # æ£€æŸ¥æ˜¯å¦ä¸ºç›®å½•
        if os.path.isdir(subfolder_path):
            rs_fish_path = os.path.join(subfolder_path, 'RS-FISH')
            
            # å¦‚æœè¯¥å­æ–‡ä»¶å¤¹åŒ…å« RS-FISH å­ç›®å½•ï¼Œåˆ™å¤„ç†å®ƒ
            if os.path.exists(rs_fish_path) and os.path.isdir(rs_fish_path):
                print(f"\nProcessing subfolder: {subfolder}")
                copy_and_rename_files(rs_fish_path, target_dir)

# è°ƒç”¨å‡½æ•°å¤„ç†æ‰€æœ‰å­æ–‡ä»¶å¤¹
process_all_subfolders(source_root_dir, target_dir)

print("\nAll files have been processed.")
```
ç»“æœå¦‚ä¸‹ï¼š    
<img src="https://github.com/y741269430/UFISH-test/blob/main/Imgs/p2.jpg" width="800" />    

### 2.3 æŠŠRS-FISHç”Ÿæˆçš„CSVç»“æœï¼Œè½¬æ¢æˆä¸¤åˆ—    
ä¸¾ä¾‹è¯´æ˜å¦‚ä½•æŠŠCSVæ–‡ä»¶å–å‡ºå…¶ä¸­ä¸¤åˆ—ï¼Œå¹¶é‡å‘½åï¼Œæ›´æ¢åˆ—é¡ºåº    
```python
import os
import pandas as pd

def rename_and_reorder_csv_columns(directory_path):
    """éå†æŒ‡å®šç›®å½•ä¸‹çš„æ‰€æœ‰ .csv æ–‡ä»¶ï¼Œå¹¶å°†åˆ—åä¿®æ”¹ä¸º axis-0 å’Œ axis-1ï¼ŒåŒæ—¶è°ƒæ•´åˆ—é¡ºåº"""
    try:
        print(f"Processing CSV files in directory: {directory_path}\n")
        
        # éå†ç›®å½•ä¸­çš„æ‰€æœ‰æ–‡ä»¶
        for file_name in os.listdir(directory_path):
            if file_name.endswith('.csv'):
                file_path = os.path.join(directory_path, file_name)
                
                try:
                    # è¯»å–CSVæ–‡ä»¶
                    df = pd.read_csv(file_path)
                    
                    # æ£€æŸ¥æ˜¯å¦æœ‰ 'x' å’Œ 'y' åˆ—
                    if 'x' not in df.columns or 'y' not in df.columns:
                        print(f"Skipping file {file_name} because it does not contain 'x' and 'y' columns.")
                        continue
                    
                    # ä¿®æ”¹åˆ—åä¸º axis-1 å’Œ axis-0
                    df.rename(columns={'y': 'axis-0', 'x': 'axis-1'}, inplace=True)
                    
                    # è°ƒæ•´åˆ—é¡ºåºï¼Œä½¿ axis-0 åœ¨ç¬¬ä¸€åˆ—ï¼Œaxis-1 åœ¨ç¬¬äºŒåˆ—
                    df = df[['axis-0', 'axis-1']]
                    
                    # ä¿å­˜ä¿®æ”¹åçš„CSVæ–‡ä»¶
                    df.to_csv(file_path, index=False)
                    print(f"Successfully processed file: {file_name}")
                
                except Exception as e:
                    print(f"Error processing file {file_path}: {e}\n")
    
    except FileNotFoundError:
        print(f"The directory {directory_path} does not exist.")
    except PermissionError:
        print(f"Permission denied accessing {directory_path}.")
    except Exception as e:
        print(f"An error occurred: {e}")

# ç¤ºä¾‹è°ƒç”¨
# è¯·æ›¿æ¢ä»¥ä¸‹å‚æ•°ä¸ºä½ å®é™…çš„ç›®å½•è·¯å¾„
directory_path = '/home/jjyang/jupyter_file/my_finetune/temp'

rename_and_reorder_csv_columns(directory_path)
```
ç»“æœå¦‚ä¸‹ï¼š    
<img src="https://github.com/y741269430/UFISH-test/blob/main/Imgs/p3.jpg" width="550" />    
<img src="https://github.com/y741269430/UFISH-test/blob/main/Imgs/p4.jpg" width="550" />   

### 2.4 è£å‰ªTIFä»¥åŠCSV    
ä¸¾ä¾‹è¯´æ˜å¦‚ä½•æŠŠæ–‡ä»¶å¤¹é‡Œé¢çš„TIFä»¥åŠCSVï¼Œè£å‰ªæˆæŒ‡å®šå¤§å°ï¼ˆ512*512ï¼‰    
```python
import os
import pandas as pd
from skimage.io import imread, imsave
import logging

# è®¾ç½®æ—¥å¿—é…ç½®
logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')

# å®šä¹‰è¾“å…¥å’Œè¾“å‡ºç›®å½•
image_input_dir = '/home/jjyang/jupyter_file/my_finetune/temp/'
csv_input_dir = image_input_dir
image_output_dir = '/home/jjyang/jupyter_file/my_finetune/temp_cut/'
csv_output_dir = image_output_dir

os.makedirs(image_output_dir, exist_ok=True)
os.makedirs(csv_output_dir, exist_ok=True)

# å®šä¹‰è£å‰ªå°ºå¯¸
crop_size = 512

def process_image_and_csv(image_path, csv_path):
    try:
        # è¯»å–åŸå§‹å›¾åƒ
        im0 = imread(image_path)
    except Exception as e:
        logging.error(f"Error reading image file {image_path}: {e}")
        return

    try:
        # è¯»å–åŸå§‹CSVæ–‡ä»¶
        df0 = pd.read_csv(csv_path, usecols=[0, 1])  # å‡è®¾CSVåªæœ‰ä¸¤åˆ—ï¼šYå’ŒX
    except Exception as e:
        logging.error(f"Error reading CSV file {csv_path}: {e}")
        return

    # æ£€æŸ¥å¹¶è½¬æ¢æ•°æ®ç±»å‹
    def convert_to_numeric(df, column):
        df.loc[:, column] = pd.to_numeric(df[column], errors='coerce')
        return df.dropna(subset=[column])
    
    df0 = convert_to_numeric(convert_to_numeric(df0, df0.columns[0]), df0.columns[1])

    total_points_before = len(df0)  # è®°å½•åŸå§‹ç‚¹çš„æ•°é‡
    logging.info(f"Total points before cropping: {total_points_before}")

    if total_points_before == 0:
        logging.warning(f"No points found in the original CSV file {csv_path}. Skipping processing.")
        return

    # è·å–å›¾åƒæ–‡ä»¶åçš„åŸºåï¼ˆä¸åŒ…æ‹¬æ‰©å±•åï¼‰
    base_name = os.path.splitext(os.path.basename(image_path))[0]

    # éå†å›¾åƒè¿›è¡Œè£å‰ª
    for start_y in range(0, im0.shape[0], crop_size):
        for start_x in range(0, im0.shape[1], crop_size):
            end_y = min(start_y + crop_size, im0.shape[0])
            end_x = min(start_x + crop_size, im0.shape[1])

            # ç¡®ä¿å½“å‰è£å‰ªåŒºåŸŸå¤§å°ä¸º512x512
            if (end_y - start_y == crop_size) and (end_x - start_x == crop_size):
                # è£å‰ªå›¾åƒ
                cropped_im0 = im0[start_y:end_y, start_x:end_x]

                # æ„å»ºè¾“å‡ºæ–‡ä»¶å
                image_filename = f"{base_name}_cropped_{start_y}_{start_x}.tif"
                image_filepath = os.path.join(image_output_dir, image_filename)

                # ä¿å­˜è£å‰ªåçš„å›¾åƒ
                try:
                    imsave(image_filepath, cropped_im0)
                    logging.info(f"Cropped image saved to {image_filepath}")
                except Exception as e:
                    logging.error(f"Error saving cropped image to {image_filepath}: {e}")
                    continue

                # è¿‡æ»¤å‡ºä½äºå½“å‰è£å‰ªåŒºåŸŸå†…çš„ç‚¹
                cropped_df = df0[(df0[df0.columns[0]] >= start_y) & (df0[df0.columns[0]] < end_y) &
                                 (df0[df0.columns[1]] >= start_x) & (df0[df0.columns[1]] < end_x)].copy()

                total_points_after = len(cropped_df)
                logging.info(f"Points before cropping: {total_points_before}, after cropping: {total_points_after}.")

                # å¦‚æœè£å‰ªåçš„DataFrameä¸ºç©ºï¼Œåˆ™è·³è¿‡ä¿å­˜
                if cropped_df.empty:
                    logging.warning("No points found in this cropped region. Skipping CSV save.")
                    continue

                # è°ƒæ•´åæ ‡ç³»
                cropped_df.loc[:, df0.columns[0]] -= start_y
                cropped_df.loc[:, df0.columns[1]] -= start_x

                # æ„å»ºè¾“å‡ºæ–‡ä»¶å
                csv_filename = f"{base_name}_cropped_{start_y}_{start_x}.csv"
                csv_filepath = os.path.join(csv_output_dir, csv_filename)

                # ä¿å­˜è°ƒæ•´åçš„CSVæ–‡ä»¶
                try:
                    cropped_df.to_csv(csv_filepath, index=False)
                    logging.info(f"Adjusted CSV saved to {csv_filepath}")
                except Exception as e:
                    logging.error(f"Error saving adjusted CSV to {csv_filepath}: {e}")

# è·å–æ‰€æœ‰å›¾åƒæ–‡ä»¶è·¯å¾„
image_files = [f for f in os.listdir(image_input_dir) if f.endswith('.tif')]

for image_file in image_files:
    # æ„å»ºå›¾åƒæ–‡ä»¶çš„å®Œæ•´è·¯å¾„
    image_path = os.path.join(image_input_dir, image_file)

    # æ ¹æ®å›¾åƒæ–‡ä»¶åæ„å»ºå¯¹åº”çš„CSVæ–‡ä»¶å
    base_name = os.path.splitext(image_file)[0]
    csv_file = f"{base_name}.csv"
    csv_path = os.path.join(csv_input_dir, csv_file)

    # æ£€æŸ¥CSVæ–‡ä»¶æ˜¯å¦å­˜åœ¨
    if not os.path.exists(csv_path):
        logging.warning(f"Warning: CSV file {csv_file} does not exist.")
        continue

    # å¤„ç†å½“å‰å›¾åƒå’ŒCSVæ–‡ä»¶
    process_image_and_csv(image_path, csv_path)

logging.info("Batch cropping and CSV processing completed.")
```
æ£€æŸ¥ä¸€ä¸‹ä¸€å…±æœ‰å¤šå°‘å¼ TIFåŠCSV
```python
import os

def count_files(directory):
    return len([item for item in os.listdir(directory) 
               if os.path.isfile(os.path.join(directory, item))])

# ç¤ºä¾‹
file_count = count_files('/home/jjyang/jupyter_file/my_finetune/temp_cut/')
print(f"æ–‡ä»¶æ•°é‡ï¼š{file_count}")
```

### 2.5 åˆ é™¤ä¸é…å¯¹çš„TIFå’ŒCSV      
ä¸Šè¿°2.4è£å‰ªäº†TIFä»¥åŠCSVï¼Œç„¶è€ŒTIFé‡Œé¢ä¸€äº›ç©ºç™½çš„åœ°æ–¹ï¼Œæ˜¯æ²¡æœ‰CSVçš„ç»“æœçš„ï¼Œæ‰€ä»¥åœ¨è£å‰ªçš„æ—¶å€™ä¼šä¸¢å¤±è¿™éƒ¨åˆ†ä¿¡æ¯ï¼Œå³TIFä¸CSVä¸å¯¹é½äº†ã€‚äºæ˜¯æˆ‘ä»¬æŠŠè¿™äº›æ²¡æœ‰å¯¹é½çš„æ–‡ä»¶è¿›è¡Œåˆ é™¤    
```python
base_directory = '/home/jjyang/jupyter_file/my_finetune/'
folder_name = 'temp_cut'

from pathlib import Path
import os

def get_base_filename(filename):
    """ä»æ–‡ä»¶åä¸­æå–åŸºç¡€éƒ¨åˆ†ï¼ˆå»æ‰æ‰©å±•åï¼‰"""
    return filename.stem

def filter_files_by_pairs(base_directory, folder_name):
    """è¿‡æ»¤æŒ‡å®šæ–‡ä»¶å¤¹ä¸­çš„æ–‡ä»¶ï¼Œåªä¿ç•™åŒæ—¶å…·æœ‰ .tif å’Œ .csv çš„æ–‡ä»¶å¯¹"""
    # æ„å»ºå®Œæ•´è·¯å¾„
    folder_path = Path(base_directory) / folder_name

    if not folder_path.exists():
        print(f"The directory {folder_path} does not exist.")
        return

    tif_files = {}
    csv_files = {}

    # æ”¶é›†æ‰€æœ‰ .tif å’Œ .csv æ–‡ä»¶
    for item in folder_path.iterdir():
        if item.is_file():
            base_name = get_base_filename(item)
            if base_name:
                if item.suffix == '.tif':
                    tif_files[base_name] = item
                elif item.suffix == '.csv':
                    csv_files[base_name] = item

    # æ‰¾å‡ºåŒæ—¶å­˜åœ¨ .tif å’Œ .csv çš„æ–‡ä»¶å¯¹
    paired_bases = set(tif_files.keys()) & set(csv_files.keys())

    # åˆ é™¤é‚£äº›æ²¡æœ‰é…å¯¹çš„æ–‡ä»¶
    for base_name, file_path in list(tif_files.items()) + list(csv_files.items()):
        if base_name not in paired_bases:
            try:
                os.remove(file_path)
                print(f"Deleted unpaired file: {file_path}")
            except Exception as e:
                print(f"Error deleting file {file_path}: {e}")

    # æ‰“å°ä¿ç•™çš„æ–‡ä»¶å¯¹
    for base_name in paired_bases:
        print(f"Kept paired files:")
        print(f"  TIF: {tif_files[base_name]}")
        print(f"  CSV: {csv_files[base_name]}")

filter_files_by_pairs(base_directory, folder_name)
```
ç»“æœå¦‚ä¸‹ï¼š    
<img src="https://github.com/y741269430/UFISH-test/blob/main/Imgs/p5.jpg" width="600" />  


### 2.6    
```python
import os
import random
import shutil
from collections import defaultdict
import sys

# è‡ªå®šä¹‰çš„æ—¥å¿—æ–‡ä»¶ç±»ï¼Œç”¨äºé‡å®šå‘ stdout
class Logger:
    def __init__(self, log_file):
        self.terminal = sys.stdout
        self.log = open(log_file, "w")

    def write(self, message):
        self.terminal.write(message)
        self.log.write(message)

    def flush(self):
        pass

# è®¾ç½®æ—¥å¿—æ–‡ä»¶è·¯å¾„
log_path = '/home/jjyang/jupyter_file/finetune_model/dataset_split/output.log'
os.makedirs(os.path.dirname(log_path), exist_ok=True)
sys.stdout = Logger(log_path)

# ä¸‹é¢æ˜¯åŸæœ‰é€»è¾‘çš„å‡½æ•°å’Œä¸»æµç¨‹
def get_unique_sample_names(folder_path):
    sample_files = defaultdict(list)
    
    for filename in os.listdir(folder_path):
        if filename.lower().endswith(('.tif', '.tiff', '.jpg', '.jpeg', '.png', '.bmp', '.csv')):
            name_part = filename.rsplit('.', 1)[0]
            if "_cropped_" in name_part:
                sample_name = name_part.split("_cropped_")[0]
                sample_files[sample_name].append(filename)
    
    return sample_files

def split_samples(sample_dict, test_ratio=0.2, train_ratio=0.6, val_ratio=0.2):
    assert test_ratio + train_ratio + val_ratio == 1.0, "æ¯”ä¾‹æ€»å’Œå¿…é¡»ä¸º1"

    samples = list(sample_dict.keys())
    random.shuffle(samples)

    total = len(samples)
    test_end = int(total * test_ratio)
    train_end = test_end + int(total * train_ratio)

    test_samples = samples[:test_end]
    train_samples = samples[test_end:train_end]
    val_samples = samples[train_end:]

    return {
        'test': test_samples,
        'train': train_samples,
        'val': val_samples
    }

def copy_samples_to_folders(sample_dict, folder_path, split_result, target_base_dir):
    # åˆ›å»ºç›®æ ‡æ–‡ä»¶å¤¹
    for folder in ['mytest', 'mytrain', 'myval']:
        os.makedirs(os.path.join(target_base_dir, folder), exist_ok=True)

    # å¤åˆ¶æ–‡ä»¶
    for folder, sample_list in split_result.items():
        for sample_name in sample_list:
            for filename in sample_dict[sample_name]:
                src = os.path.join(folder_path, filename)
                dst = os.path.join(target_base_dir, folder, filename)
                shutil.copy(src, dst)  # å¦‚æœæ˜¯ç§»åŠ¨è€Œä¸æ˜¯å¤åˆ¶ï¼Œç”¨ shutil.move
                print(f"å·²å¤åˆ¶ {filename} åˆ° {folder}")

# ä½¿ç”¨ç¤ºä¾‹
folder_path = '/home/jjyang/jupyter_file/my_finetune/temp_cut/'
target_base_dir = '/home/jjyang/jupyter_file/finetune_model/'

print("ğŸ” æ­£åœ¨æ”¶é›†æ ·æœ¬...")
sample_dict = get_unique_sample_names(folder_path)

print("ğŸ”„ æ­£åœ¨åˆ’åˆ†æ ·æœ¬...")
split_result = split_samples(sample_dict)

print("ğŸšš æ­£åœ¨å¤åˆ¶æ–‡ä»¶...")
copy_samples_to_folders(sample_dict, folder_path, split_result, target_base_dir)

print("âœ… å®Œæˆï¼")
```



## 3.UFISH Finetune ####
```
cd UD2_finetune
```
#### 3.1 ç›´æ¥è¿è¡Œé»˜è®¤æ¨¡å‹ ####
```
nohup ufish load_weights v1.0-alldata-ufish_c32.pth - predict_imgs ../FISH_spots/UD2_datasets/text predict --img_glob=\"*.tif\" --intensity_threshold 0.5 &
```
#### 3.2 Finetune ####  
``` 
CUDA_VISIBLE_DEVICES=0 nohup ufish load-weights ../v1.0-alldata-ufish_c32.pth \
- train -n 100 \
../FISH_spots/UD2_datasets/train 100 \
../FISH_spots/UD2_datasets/val \
--model_save_dir ./model_finetune/ \
--batch_size 16 >> train_output.log 2>&1 &
```
è¾“å‡ºæ—¥å¿—è½¬æ¢æˆtxtç”¨äºç»˜å›¾ï¼š
```
grep -E "Train Loss.*Val Loss" train_output.log  > train_val_loss.txt
```
æ¨¡å‹å­˜æ”¾äºæ­¤ï¼š
```
ls model_finetune
```
#### 3.2.1 Check your fine-tuned model ####  
```python
import re
import matplotlib.pyplot as plt

epochs = []
train_loss = []
val_loss = []

pattern = r"Epoch (\d+)/\d+, Train Loss: ([\d\.]+), Val Loss: ([\d\.]+)"

with open("./train_val_loss.txt", "r") as file:
    for line in file:
        match = re.search(pattern, line)
        if match:
            epoch = int(match.group(1))  # æå– Epoch
            train_loss_value = float(match.group(2))  # æå– Train Loss
            val_loss_value = float(match.group(3))  # æå– Val Loss
            epochs.append(epoch)
            train_loss.append(train_loss_value)
            val_loss.append(val_loss_value)

plt.figure(figsize=(8, 5))
plt.plot(epochs, train_loss, 'b', label='Train Loss')
plt.plot(epochs, val_loss, 'r', label='Val Loss')

plt.title('Training vs Validation Loss')
plt.xlabel('Epochs')
plt.ylabel('Loss')

plt.grid(True)
plt.legend()

plt.xlim(0, max(epochs))
plt.xticks(range(0, 101, 5))

# å¦‚æœæœ‰æœ€ä½³éªŒè¯æŸå¤±çš„ Epochï¼Œå¯ä»¥æ·»åŠ ä¸€æ¡ç«–çº¿æ ‡è®°
# best_epoch = 80  # æœ€ä½³ Epoch
# plt.axvline(x=best_epoch, color='g', linestyle='--', label=f'Best Val Epoch ({best_epoch})')

# save_path = "./loss_plot.pdf"
# plt.savefig(save_path, dpi=300, bbox_inches='tight') 

# æ˜¾ç¤ºå›¾è¡¨
plt.show()
```
#### 3.3 Prediction & Evaluation ####  
è®­ç»ƒå®Œåï¼Œæ ¹æ®ä¸Šè¿°å›¾è¡¨å¯»æ‰¾æœ€ä¼˜çš„æ¨¡å‹ (e.g., `model_80.pth`) 
```
# run prediction 
nohup ufish load_weights ./model_finetune/model_80.pth \
- predict_imgs ../FISH_spots/UD2_datasets/test \
./predict_test --img_glob="*.tif" \
--intensity_threshold 0.1 \
--axes='yx' &
```
```
# run evaluation
nohup ufish evaluate_imgs ./predict_test ../FISH_spots/UD2_datasets/test ./eval_UD2_text.csv &
```
è·å–`eval_UD2_text.csv`åï¼Œå¯ä½¿ç”¨ä»¥ä¸‹ä»£ç è¾“å‡ºå¹³å‡F1 score
```python
import pandas as pd
eval_df = pd.read_csv("./eval.csv")
eval_df = eval_df[eval_df['true num'] != 0]  # remove empty image
print("Mean f1(cutoff=3.0):", eval_df['f1(cutoff=3)'].mean())
```
#### 3.4 Train a new model ####
ä½¿ç”¨ä»¥ä¸‹ä»£ç æ‰§è¡Œé‡å¤´è®­ç»ƒï¼ˆè·å¾—ä¸€ä¸ªæ–°çš„æ¨¡å‹ï¼‰
```
CUDA_VISIBLE_DEVICES=0 nohup ufish train \
- train -n 100 \
../FISH_spots/UD2_datasets/train 100 \
../FISH_spots/UD2_datasets/val \
--model_save_dir ./new_model/ \
--batch_size 16 >> train_output.log 2>&1 &
```
è¿è¡Œå®Œæ¯•åï¼Œæ‰§è¡Œä»¥ä¸‹å‘½ä»¤è·å–æ—¥å¿—
```
grep -E "Train Loss.*Val Loss" train_output.log  > train_val_loss.txt
```
æ¥ä¸‹æ¥ä¾æ¬¡æ‰§è¡Œ 3.2.1, 3.3.

---
## 4.Spotiflow Finetune ####
```
cd spt_finetune
```
#### 4.1 ç›´æ¥è¿è¡Œé»˜è®¤æ¨¡å‹ ####
```
spotiflow-predict ../FISH_spots/UD2_datasets/test -pm general -o predict -t 0.5 -d gpu &
```
#### 4.2 Finetune ####  
ç¬¬ä¸€æ¬¡æ‰§è¡Œçš„æ—¶å€™ï¼Œä¼šä¸‹è½½æ¨¡å‹åŠæ•°æ®é›†ï¼Œå­˜æ”¾äº`~/.spotiflow/models/general`
```
CUDA_VISIBLE_DEVICES=0 nohup spotiflow-train ../FISH_spots/UD2_datasets/ \
--finetune-from ./general \
-o model_finetune \
--batch-size 4 \
--device cuda \
--num-epochs 100 >> train_output.log 2>&1 &
```
ä½¿ç”¨ä»¥ä¸‹ä»£ç è½¬æ¢è¾“å‡ºçš„log:
```python
import pandas as pd
import re

# æ–‡ä»¶è·¯å¾„
log_file = "./train_output.log"
output_csv = "./train_val_loss.txt"

# è§£æå‡½æ•°
def parse_log(file_path):
    pattern = re.compile(
        r"Epoch (\d+):.*?" +
        r"val_loss=([\d\.]+).*?" +
        r"val_f1=([\d\.]+).*?" +
        r"val_acc=([\d\.]+).*?" +
        r"heatmap_loss=([\d\.]+).*?" +
        r"flow_loss=([\d\.]+).*?" +
        r"train_loss=([\d\.]+)"
    )
    
    records = []
    with open(file_path, 'r') as f:
        for line in f:
            match = pattern.search(line.strip())  # ä½¿ç”¨searchè€Œä¸æ˜¯match
            if match:
                records.append({
                    'Epoch': int(match.group(1)),
                    'val_loss': float(match.group(2)),
                    'val_f1': float(match.group(3)),
                    'val_acc': float(match.group(4)),
                    'heatmap_loss': float(match.group(5)),
                    'flow_loss': float(match.group(6)),
                    'train_loss': float(match.group(7))
                })
    return pd.DataFrame(records)
try:
    df = parse_log(log_file)
    if not df.empty:
        df_unique = df.drop_duplicates(subset=['Epoch'], keep='last')
        df_unique.to_csv(output_csv, index=False)
        print("å¤„ç†æˆåŠŸï¼Œç»“æœå·²ä¿å­˜åˆ°:", output_csv)
        print(df_unique)
    else:
        print("è­¦å‘Š: æ²¡æœ‰æ‰¾åˆ°ä»»ä½•åŒ¹é…çš„Epochæ•°æ®ï¼")
except Exception as e:
    print("å‘ç”Ÿé”™è¯¯:", str(e))
```
#### 4.2.1 Check your fine-tuned model ####  
```python
import pandas as pd
import matplotlib.pyplot as plt

# è¯»å–CSVæ–‡ä»¶
file_path = "./train_val_loss.txt"
data = pd.read_csv(file_path)

columns_to_plot = ['Epoch', 'val_loss', 'train_loss']

plt.figure(figsize=(9, 5))

for column in columns_to_plot[1:]:
    plt.plot(data['Epoch'], data[column], label=column)

plt.title('Training Metrics Over Epochs')
plt.xlabel('Epoch')
plt.ylabel('Value')
plt.grid(True)
plt.legend()

plt.xlim(0, 100)
plt.xticks(range(0, 101, 5))

#save_path = "./loss_plot.pdf"
#plt.savefig(save_path, dpi=300, bbox_inches='tight') 

plt.show()
```
#### 4.3 Prediction & Evaluation ####  
```
# run prediction 
CUDA_VISIBLE_DEVICES=0 nohup spotiflow-predict ../FISH_spots/UD2_datasets/test \
-md ./model_finetune -o ./predict_test -t 0.5 -d cuda &
```
ä½¿ç”¨è„šæœ¬å¯¹è¾“å‡ºcsvè¿›è¡Œæ ¼å¼è½¬æ¢
```
# run evaluation 1 (Processing... Please wait.)
bash spt2pred.sh ./predict_test
```
```
# run evaluation 2
nohup ufish evaluate_imgs ./predict_test ../FISH_spots/UD2_datasets/test ./eval_UD2_text.csv &
```

---
## 5.deepBlink Finetune ####
```
cd dpl_finetune
```
#### 5.1 ç›´æ¥è¿è¡Œé»˜è®¤æ¨¡å‹ ####
ä½¿ç”¨ä»¥ä¸‹ä»£ç ä¸‹è½½æ¨¡å‹ï¼Œæˆ–ç½‘ç«™ä¸‹è½½[here](https://figshare.com/articles/software/Download/13220717)
```
deepblink download
```
```
deepblink predict --model dpbl_model/smfish.h5 -i ../FISH_spots/UD2_datasets/test -o ./predict &
```
#### 5.2 Predict ####  
```
nohup deepblink predict --model ./230903_210048_deepBlink_is_sweet.h5 -i ../FISH_spots/UD2_datasets/test/ -o ./predict_test/ &
```
è¿™é‡Œéœ€è¦æ‰‹åŠ¨åˆ é™¤ä¸€äº›åªæœ‰è¡¨å¤´çš„æ–‡ä»¶ï¼Œä¸ç„¶æ‰§è¡Œä»¥ä¸‹ä»£ç ä¼šæŠ¥é”™
```
# æ£€æŸ¥åªæœ‰è¡¨å¤´çš„æ–‡ä»¶
wc -l predict/*.csv | awk '$1 < 3 && $1 != "total" {print $2}'
# ç„¶åæŠŠè¿™äº›åªæœ‰è¡¨å¤´çš„æ–‡ä»¶åˆ é™¤
wc -l predict/*.csv | awk '$1 < 3 && $1 != "total" {system("rm -f "$2)}'
```
ä½¿ç”¨è„šæœ¬å¯¹è¾“å‡ºcsvè¿›è¡Œæ ¼å¼è½¬æ¢
```
# å¯¹è¾“å‡ºçš„ç»“æœè¿›è¡Œè½¬æ¢
python dpl2pred.py
```
```
# run evaluation
nohup ufish evaluate_imgs ./predict_test ../FISH_spots/UD2_datasets/test ./eval_raw.csv &
```
#### 5.3 Train ####
#### 5.3.1 Trans UD2_datasets to npz ####
ä½¿ç”¨ä»¥ä¸‹è„šæœ¬å¯¹`UD2_datasets`è½¬æ¢ä¸º`deepblink.npz`
```python
import os
import numpy as np
from skimage import io
import pandas as pd

# æ•°æ®æ ¹ç›®å½•
base_path = './FISH_spots/UD2_datasets/'

# è¾“å‡ºæ–‡ä»¶å
output_name = 'deepblink'

def load_dataset_from_folder(folder_path):
    """
    åŠ è½½æŒ‡å®šæ–‡ä»¶å¤¹ä¸‹çš„æ‰€æœ‰ tif/csv å¯¹åº”æ•°æ®
    """
    image_files = [f for f in os.listdir(folder_path) if f.endswith('.tif')]
    images = []
    labels = []

    for img_file in image_files:
        base_name = os.path.splitext(img_file)[0]
        csv_file = base_name + '.csv'
        img_path = os.path.join(folder_path, img_file)
        csv_path = os.path.join(folder_path, csv_file)

        # è¯»å–å›¾åƒ
        img = io.imread(img_path)
        images.append(img)

        # è¯»å– CSV æ ‡æ³¨ç‚¹
        try:
            df = pd.read_csv(csv_path, header=1)
            labels.append(df.values)
        except Exception as e:
            print(f"Error reading {csv_path}: {e}")
            labels.append(np.zeros(shape=(0, 2)))

    return np.array(images), np.array(labels, dtype=object)

# åˆ†åˆ«åŠ è½½ä¸‰ä¸ªé›†åˆ
train_data, train_labels = load_dataset_from_folder(os.path.join(base_path, 'train'))
valid_data, valid_labels = load_dataset_from_folder(os.path.join(base_path, 'val'))
test_data, test_labels = load_dataset_from_folder(os.path.join(base_path, 'test'))

# ä¿å­˜ä¸º .npz æ–‡ä»¶
np.savez(
    f"./{output_name}.npz",
    x_train=train_data,
    y_train=train_labels,
    x_valid=valid_data,
    y_valid=valid_labels,
    x_test=test_data,
    y_test=test_labels
)

print("âœ… æ•°æ®é›†å·²æˆåŠŸåŠ è½½å¹¶ä¿å­˜ï¼")
```
#### 5.3.2 Train ####
æ‰§è¡Œä»¥ä¸‹ä»£ç ï¼Œä¼šç”Ÿæˆä¸€ä¸ª`config.yaml`ï¼Œç„¶åè¿›å»å¡«å†™è·¯å¾„ï¼Œbetchsizeç­‰ä¿¡æ¯
```
deepblink config
```
è¿è¡Œ
```
nohup deepblink train --config config.yaml -g 0 &
```
#### 5.4 Prediction & Evaluation ####  
```
nohup deepblink predict --model ./fine_md/250610_224839_deepBlink_is_sweet.h5 -i ../FISH_spots/UD2_datasets/test/ -o ./predict_finetune/ &
```
è¿™é‡Œéœ€è¦æ‰‹åŠ¨åˆ é™¤ä¸€äº›åªæœ‰è¡¨å¤´çš„æ–‡ä»¶ï¼Œä¸ç„¶æ‰§è¡Œä»¥ä¸‹ä»£ç ä¼šæŠ¥é”™
```
# æ£€æŸ¥åªæœ‰è¡¨å¤´çš„æ–‡ä»¶
wc -l predict_finetune/*.csv | awk '$1 < 3 && $1 != "total" {print $2}'
# ç„¶åæŠŠè¿™äº›åªæœ‰è¡¨å¤´çš„æ–‡ä»¶åˆ é™¤
wc -l predict_finetune/*.csv | awk '$1 < 3 && $1 != "total" {system("rm -f "$2)}'
```
ä½¿ç”¨è„šæœ¬å¯¹è¾“å‡ºcsvè¿›è¡Œæ ¼å¼è½¬æ¢
```
# å¯¹è¾“å‡ºçš„ç»“æœè¿›è¡Œè½¬æ¢
python dpl2pred.py
```
```
# run evaluation
nohup ufish evaluate_imgs ./predict_finetune ../FISH_spots/UD2_datasets/test ./eval_UD2_text.csv &
```

---
#### æ¯”è¾ƒ F1 score æˆ– Mean distance ####
```python
import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# å®šä¹‰æ–‡ä»¶è·¯å¾„å’Œæ¨¡å‹åç§°
data_sources = [
    ("./UD2_finetune/eval_raw.csv", "U-fish") ,
    ("./UD2_finetune/eval_UD2_text.csv", "U-fish Finetune"),
    ("./spt_finetune/eval_raw.csv", "Spotiflow"),
    ("./spt_finetune/eval_UD2_text.csv", "Spotiflow Finetune"),
    ("./dp_finetune/eval_raw.csv", "deepBlink"),
    ("./dp_finetune/eval_UD2_text.csv", "deepBlink Finetune"),
    ("./new-ud2/eval_UD2_text.csv", "Big-FISH"),
    ("./new-ud2/eval_UD2_text.csv", "RS-FISH"),
    ("./new-ud2/eval_UD2_text.csv", "Starfish"),
    ("./new-ud2/eval_UD2_text.csv", "TrackMate")
]

# å®šä¹‰ç›®æ ‡åˆ—å
source_column_name = 'source'

# å®šä¹‰éœ€è¦ç»˜åˆ¶çš„æŒ‡æ ‡
target_metrics = ["f1(cutoff=3)"]
#target_metrics = ["mean distance"]

# åˆå§‹åŒ–ä¸€ä¸ªç©ºçš„ DataFrame ç”¨äºå­˜å‚¨åˆå¹¶æ•°æ®
all_data = pd.DataFrame()

# è¯»å–å¹¶åˆå¹¶æ‰€æœ‰ CSV æ•°æ®
for file, model_name in data_sources:
    data = pd.read_csv(file)
    data['model'] = model_name  # æ·»åŠ æ¨¡å‹åç§°
    all_data = pd.concat([all_data, data], ignore_index=True)

# æŒ‰ç…§'source'åˆ—çš„é¦–å­—æ¯é¡ºåºå¯¹æ•°æ®è¿›è¡Œæ’åº
all_data[source_column_name] = all_data[source_column_name].astype(str)
all_data = all_data.sort_values(by=source_column_name, key=lambda col: col.str.lower())

# è®¾ç½®æ¨¡å‹é¡ºåº

model_order = ["U-fish", "U-fish Finetune",  "Spotiflow", "Spotiflow Finetune", "deepBlink", "deepBlink Finetune", "Big-FISH", "RS-FISH", "Starfish", "TrackMate"]

# å¾ªç¯ç»˜åˆ¶æ¯ä¸ªæŒ‡æ ‡çš„ç®±çº¿å›¾
for target_metric in target_metrics:
    plt.figure(figsize=(11,6))
    sns.boxplot(
        x=source_column_name, 
        y=target_metric, 
        hue="model",  # ä½¿ç”¨æ¨¡å‹åç§°åŒºåˆ†ä¸åŒæ•°æ®æ¥æº
        data=all_data,
        showmeans=False,  # æ˜¾ç¤ºå‡å€¼ç‚¹
        meanprops={'marker':'D', 'markerfacecolor':'red'},  # çº¢è‰²è±å½¢è¡¨ç¤ºå‡å€¼
        palette="tab10",  # è®¾ç½®é¢œè‰²è°ƒè‰²æ¿
        hue_order=model_order  # è®¾ç½®æ¨¡å‹é¡ºåº
    )

    # æ·»åŠ ä¸€æ¡æ°´å¹³çº¿ (y=0.85)
    #plt.axhline(y=0.85, color='r', linestyle='--', linewidth=1.5, label='Threshold (0.85)')
    
    # è®¾ç½® y è½´åˆ»åº¦
    if target_metric == "f1(cutoff=3)":
        plt.yticks([0,0.1,0.2,0.3,0.4,0.5,0.6,0.7,0.8,0.85,0.9,0.95,1]) 
    elif target_metric == "mean distance":
        plt.yticks([0, 1]) 

    # è®¾ç½®æ ‡é¢˜
    if target_metric == "f1(cutoff=3)":
        plt.title("Performance comparison on UD2")
    elif target_metric == "mean distance":
        plt.title("Performance comparison on UD2")

    # å…¶ä»–è®¾ç½®
    plt.xticks(rotation=45)
    plt.grid(alpha=0.3)
    plt.legend(bbox_to_anchor=(1.4, 1), loc = 'upper right')  # æ˜¾ç¤ºå›¾ä¾‹ï¼ŒåŒ…å«æ°´å¹³çº¿æ ‡ç­¾
    plt.tight_layout()

    #save_path = "/home/jjyang/jupyter_file/boxplot_ud2_all.pdf"
    #plt.savefig(save_path, dpi=300, bbox_inches='tight') 

    # æ˜¾ç¤ºå›¾åƒ
    plt.show()
```





